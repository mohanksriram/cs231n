{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.314087\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *We are generating a random set of weights, where in there is no bias towards any of the classes.\n",
    "Logarithmic outputs would be uniformly distributed and subsequently the probabilities from the softmax function.\n",
    "In that sense, we have probability for all class outputs as x and thus x/10x -> 0.1; which leads to -log(0.1)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.975745 analytic: -1.975745, relative error: 3.831230e-09\n",
      "numerical: -0.987887 analytic: -0.987887, relative error: 6.681961e-09\n",
      "numerical: 0.974653 analytic: 0.974653, relative error: 4.451949e-08\n",
      "numerical: -3.017296 analytic: -3.017296, relative error: 1.800687e-08\n",
      "numerical: 0.385274 analytic: 0.385273, relative error: 1.600924e-07\n",
      "numerical: -0.116387 analytic: -0.116388, relative error: 4.581542e-07\n",
      "numerical: -1.902230 analytic: -1.902230, relative error: 2.519982e-09\n",
      "numerical: 1.751521 analytic: 1.751521, relative error: 1.763840e-08\n",
      "numerical: -0.163124 analytic: -0.163124, relative error: 3.930992e-07\n",
      "numerical: -1.000135 analytic: -1.000135, relative error: 2.470628e-08\n",
      "numerical: -0.236793 analytic: -0.236793, relative error: 7.161416e-08\n",
      "numerical: 1.479645 analytic: 1.479645, relative error: 2.684378e-08\n",
      "numerical: -0.624705 analytic: -0.624705, relative error: 1.302584e-07\n",
      "numerical: -1.453038 analytic: -1.453038, relative error: 2.646175e-08\n",
      "numerical: 3.692703 analytic: 3.692703, relative error: 4.275185e-09\n",
      "numerical: -2.440871 analytic: -2.440871, relative error: 7.600615e-09\n",
      "numerical: -0.640005 analytic: -0.640005, relative error: 7.588308e-09\n",
      "numerical: -1.126877 analytic: -1.126877, relative error: 1.015679e-08\n",
      "numerical: -0.147587 analytic: -0.147587, relative error: 5.146562e-07\n",
      "numerical: -0.088607 analytic: -0.088607, relative error: 4.448857e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.314087e+00 computed in 0.125769s\n",
      "vectorized loss: 2.314087e+00 computed in 0.003876s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 765.850434\n",
      "iteration 100 / 2000: loss 281.269361\n",
      "iteration 200 / 2000: loss 104.066739\n",
      "iteration 300 / 2000: loss 39.415267\n",
      "iteration 400 / 2000: loss 15.798541\n",
      "iteration 500 / 2000: loss 7.018833\n",
      "iteration 600 / 2000: loss 3.954360\n",
      "iteration 700 / 2000: loss 2.736255\n",
      "iteration 800 / 2000: loss 2.314856\n",
      "iteration 900 / 2000: loss 2.140423\n",
      "iteration 1000 / 2000: loss 2.153624\n",
      "iteration 1100 / 2000: loss 2.120686\n",
      "iteration 1200 / 2000: loss 2.163341\n",
      "iteration 1300 / 2000: loss 2.070437\n",
      "iteration 1400 / 2000: loss 2.127160\n",
      "iteration 1500 / 2000: loss 2.001683\n",
      "iteration 1600 / 2000: loss 2.070450\n",
      "iteration 1700 / 2000: loss 2.145812\n",
      "iteration 1800 / 2000: loss 2.096236\n",
      "iteration 1900 / 2000: loss 2.041145\n",
      "That took 4.479140s\n",
      "iteration 0 / 2000: loss 959.461680\n",
      "iteration 100 / 2000: loss 274.200732\n",
      "iteration 200 / 2000: loss 79.599529\n",
      "iteration 300 / 2000: loss 24.180412\n",
      "iteration 400 / 2000: loss 8.373745\n",
      "iteration 500 / 2000: loss 3.929978\n",
      "iteration 600 / 2000: loss 2.620623\n",
      "iteration 700 / 2000: loss 2.279488\n",
      "iteration 800 / 2000: loss 2.146843\n",
      "iteration 900 / 2000: loss 2.099479\n",
      "iteration 1000 / 2000: loss 2.141121\n",
      "iteration 1100 / 2000: loss 2.075717\n",
      "iteration 1200 / 2000: loss 2.107200\n",
      "iteration 1300 / 2000: loss 2.102335\n",
      "iteration 1400 / 2000: loss 2.146807\n",
      "iteration 1500 / 2000: loss 2.094310\n",
      "iteration 1600 / 2000: loss 2.146429\n",
      "iteration 1700 / 2000: loss 2.079404\n",
      "iteration 1800 / 2000: loss 2.059117\n",
      "iteration 1900 / 2000: loss 2.064667\n",
      "That took 4.449809s\n",
      "iteration 0 / 2000: loss 1171.549500\n",
      "iteration 100 / 2000: loss 260.422980\n",
      "iteration 200 / 2000: loss 59.296138\n",
      "iteration 300 / 2000: loss 14.747639\n",
      "iteration 400 / 2000: loss 4.900183\n",
      "iteration 500 / 2000: loss 2.744990\n",
      "iteration 600 / 2000: loss 2.285873\n",
      "iteration 700 / 2000: loss 2.187155\n",
      "iteration 800 / 2000: loss 2.090572\n",
      "iteration 900 / 2000: loss 2.115846\n",
      "iteration 1000 / 2000: loss 2.184862\n",
      "iteration 1100 / 2000: loss 2.120179\n",
      "iteration 1200 / 2000: loss 2.051439\n",
      "iteration 1300 / 2000: loss 2.091715\n",
      "iteration 1400 / 2000: loss 2.136595\n",
      "iteration 1500 / 2000: loss 2.084977\n",
      "iteration 1600 / 2000: loss 2.163710\n",
      "iteration 1700 / 2000: loss 2.136760\n",
      "iteration 1800 / 2000: loss 2.150534\n",
      "iteration 1900 / 2000: loss 2.110592\n",
      "That took 4.710239s\n",
      "iteration 0 / 2000: loss 1337.188918\n",
      "iteration 100 / 2000: loss 231.252246\n",
      "iteration 200 / 2000: loss 41.519102\n",
      "iteration 300 / 2000: loss 8.886145\n",
      "iteration 400 / 2000: loss 3.263463\n",
      "iteration 500 / 2000: loss 2.315569\n",
      "iteration 600 / 2000: loss 2.163178\n",
      "iteration 700 / 2000: loss 2.124049\n",
      "iteration 800 / 2000: loss 2.177526\n",
      "iteration 900 / 2000: loss 2.084266\n",
      "iteration 1000 / 2000: loss 2.124611\n",
      "iteration 1100 / 2000: loss 2.155818\n",
      "iteration 1200 / 2000: loss 2.141036\n",
      "iteration 1300 / 2000: loss 2.163478\n",
      "iteration 1400 / 2000: loss 2.136558\n",
      "iteration 1500 / 2000: loss 2.135335\n",
      "iteration 1600 / 2000: loss 2.120825\n",
      "iteration 1700 / 2000: loss 2.147626\n",
      "iteration 1800 / 2000: loss 2.179241\n",
      "iteration 1900 / 2000: loss 2.089393\n",
      "That took 4.587576s\n",
      "iteration 0 / 2000: loss 1544.170635\n",
      "iteration 100 / 2000: loss 207.925394\n",
      "iteration 200 / 2000: loss 29.625636\n",
      "iteration 300 / 2000: loss 5.829565\n",
      "iteration 400 / 2000: loss 2.668493\n",
      "iteration 500 / 2000: loss 2.204135\n",
      "iteration 600 / 2000: loss 2.169036\n",
      "iteration 700 / 2000: loss 2.187763\n",
      "iteration 800 / 2000: loss 2.161966\n",
      "iteration 900 / 2000: loss 2.116932\n",
      "iteration 1000 / 2000: loss 2.158025\n",
      "iteration 1100 / 2000: loss 2.207329\n",
      "iteration 1200 / 2000: loss 2.082197\n",
      "iteration 1300 / 2000: loss 2.121293\n",
      "iteration 1400 / 2000: loss 2.115409\n",
      "iteration 1500 / 2000: loss 2.130852\n",
      "iteration 1600 / 2000: loss 2.206128\n",
      "iteration 1700 / 2000: loss 2.159621\n",
      "iteration 1800 / 2000: loss 2.130178\n",
      "iteration 1900 / 2000: loss 2.143526\n",
      "That took 4.649325s\n",
      "iteration 0 / 2000: loss 765.996191\n",
      "iteration 100 / 2000: loss 103.600704\n",
      "iteration 200 / 2000: loss 15.645057\n",
      "iteration 300 / 2000: loss 3.881760\n",
      "iteration 400 / 2000: loss 2.286311\n",
      "iteration 500 / 2000: loss 2.060620\n",
      "iteration 600 / 2000: loss 2.129643\n",
      "iteration 700 / 2000: loss 2.064078\n",
      "iteration 800 / 2000: loss 2.134669\n",
      "iteration 900 / 2000: loss 1.987153\n",
      "iteration 1000 / 2000: loss 2.108318\n",
      "iteration 1100 / 2000: loss 2.027891\n",
      "iteration 1200 / 2000: loss 2.087367\n",
      "iteration 1300 / 2000: loss 2.127408\n",
      "iteration 1400 / 2000: loss 2.053532\n",
      "iteration 1500 / 2000: loss 2.061254\n",
      "iteration 1600 / 2000: loss 2.060031\n",
      "iteration 1700 / 2000: loss 2.107330\n",
      "iteration 1800 / 2000: loss 2.114402\n",
      "iteration 1900 / 2000: loss 2.083411\n",
      "That took 4.577132s\n",
      "iteration 0 / 2000: loss 974.403644\n",
      "iteration 100 / 2000: loss 80.033278\n",
      "iteration 200 / 2000: loss 8.351444\n",
      "iteration 300 / 2000: loss 2.529074\n",
      "iteration 400 / 2000: loss 2.089421\n",
      "iteration 500 / 2000: loss 2.094307\n",
      "iteration 600 / 2000: loss 2.113511\n",
      "iteration 700 / 2000: loss 2.068732\n",
      "iteration 800 / 2000: loss 2.083181\n",
      "iteration 900 / 2000: loss 2.106466\n",
      "iteration 1000 / 2000: loss 2.061159\n",
      "iteration 1100 / 2000: loss 2.116486\n",
      "iteration 1200 / 2000: loss 2.152986\n",
      "iteration 1300 / 2000: loss 2.145856\n",
      "iteration 1400 / 2000: loss 2.041711\n",
      "iteration 1500 / 2000: loss 2.105498\n",
      "iteration 1600 / 2000: loss 2.046788\n",
      "iteration 1700 / 2000: loss 2.109262\n",
      "iteration 1800 / 2000: loss 2.069648\n",
      "iteration 1900 / 2000: loss 2.076681\n",
      "That took 4.831786s\n",
      "iteration 0 / 2000: loss 1158.130138\n",
      "iteration 100 / 2000: loss 58.027265\n",
      "iteration 200 / 2000: loss 4.812857\n",
      "iteration 300 / 2000: loss 2.277174\n",
      "iteration 400 / 2000: loss 2.103984\n",
      "iteration 500 / 2000: loss 2.140041\n",
      "iteration 600 / 2000: loss 2.125808\n",
      "iteration 700 / 2000: loss 2.092924\n",
      "iteration 800 / 2000: loss 2.082761\n",
      "iteration 900 / 2000: loss 2.095356\n",
      "iteration 1000 / 2000: loss 2.126304\n",
      "iteration 1100 / 2000: loss 2.140775\n",
      "iteration 1200 / 2000: loss 2.129180\n",
      "iteration 1300 / 2000: loss 2.137581\n",
      "iteration 1400 / 2000: loss 2.107514\n",
      "iteration 1500 / 2000: loss 2.095022\n",
      "iteration 1600 / 2000: loss 2.182109\n",
      "iteration 1700 / 2000: loss 2.120291\n",
      "iteration 1800 / 2000: loss 2.103161\n",
      "iteration 1900 / 2000: loss 2.143665\n",
      "That took 4.759021s\n",
      "iteration 0 / 2000: loss 1346.351205\n",
      "iteration 100 / 2000: loss 41.219508\n",
      "iteration 200 / 2000: loss 3.298337\n",
      "iteration 300 / 2000: loss 2.158374\n",
      "iteration 400 / 2000: loss 2.189208\n",
      "iteration 500 / 2000: loss 2.096901\n",
      "iteration 600 / 2000: loss 2.142453\n",
      "iteration 700 / 2000: loss 2.120943\n",
      "iteration 800 / 2000: loss 2.138811\n",
      "iteration 900 / 2000: loss 2.103654\n",
      "iteration 1000 / 2000: loss 2.116793\n",
      "iteration 1100 / 2000: loss 2.198888\n",
      "iteration 1200 / 2000: loss 2.151415\n",
      "iteration 1300 / 2000: loss 2.149817\n",
      "iteration 1400 / 2000: loss 2.109780\n",
      "iteration 1500 / 2000: loss 2.145268\n",
      "iteration 1600 / 2000: loss 2.142587\n",
      "iteration 1700 / 2000: loss 2.124847\n",
      "iteration 1800 / 2000: loss 2.134301\n",
      "iteration 1900 / 2000: loss 2.147375\n",
      "That took 4.736840s\n",
      "iteration 0 / 2000: loss 1526.102626\n",
      "iteration 100 / 2000: loss 28.763845\n",
      "iteration 200 / 2000: loss 2.612216\n",
      "iteration 300 / 2000: loss 2.192469\n",
      "iteration 400 / 2000: loss 2.125521\n",
      "iteration 500 / 2000: loss 2.159572\n",
      "iteration 600 / 2000: loss 2.183430\n",
      "iteration 700 / 2000: loss 2.098187\n",
      "iteration 800 / 2000: loss 2.188219\n",
      "iteration 900 / 2000: loss 2.164427\n",
      "iteration 1000 / 2000: loss 2.130147\n",
      "iteration 1100 / 2000: loss 2.161479\n",
      "iteration 1200 / 2000: loss 2.124145\n",
      "iteration 1300 / 2000: loss 2.162533\n",
      "iteration 1400 / 2000: loss 2.154085\n",
      "iteration 1500 / 2000: loss 2.216029\n",
      "iteration 1600 / 2000: loss 2.175679\n",
      "iteration 1700 / 2000: loss 2.106360\n",
      "iteration 1800 / 2000: loss 2.154645\n",
      "iteration 1900 / 2000: loss 2.112692\n",
      "That took 4.483492s\n",
      "iteration 0 / 2000: loss 774.470014\n",
      "iteration 100 / 2000: loss 39.330833\n",
      "iteration 200 / 2000: loss 3.842366\n",
      "iteration 300 / 2000: loss 2.148013\n",
      "iteration 400 / 2000: loss 2.154505\n",
      "iteration 500 / 2000: loss 2.085755\n",
      "iteration 600 / 2000: loss 2.114426\n",
      "iteration 700 / 2000: loss 2.037365\n",
      "iteration 800 / 2000: loss 2.103654\n",
      "iteration 900 / 2000: loss 2.084411\n",
      "iteration 1000 / 2000: loss 2.069236\n",
      "iteration 1100 / 2000: loss 2.057874\n",
      "iteration 1200 / 2000: loss 2.095828\n",
      "iteration 1300 / 2000: loss 2.011712\n",
      "iteration 1400 / 2000: loss 1.993379\n",
      "iteration 1500 / 2000: loss 2.103741\n",
      "iteration 1600 / 2000: loss 2.060919\n",
      "iteration 1700 / 2000: loss 2.091115\n",
      "iteration 1800 / 2000: loss 2.100388\n",
      "iteration 1900 / 2000: loss 2.073858\n",
      "That took 4.334496s\n",
      "iteration 0 / 2000: loss 970.634169\n",
      "iteration 100 / 2000: loss 23.868435\n",
      "iteration 200 / 2000: loss 2.583222\n",
      "iteration 300 / 2000: loss 2.149023\n",
      "iteration 400 / 2000: loss 2.054903\n",
      "iteration 500 / 2000: loss 2.075219\n",
      "iteration 600 / 2000: loss 2.097215\n",
      "iteration 700 / 2000: loss 2.143556\n",
      "iteration 800 / 2000: loss 2.085053\n",
      "iteration 900 / 2000: loss 2.045604\n",
      "iteration 1000 / 2000: loss 2.058346\n",
      "iteration 1100 / 2000: loss 2.126918\n",
      "iteration 1200 / 2000: loss 2.094113\n",
      "iteration 1300 / 2000: loss 2.064891\n",
      "iteration 1400 / 2000: loss 2.156288\n",
      "iteration 1500 / 2000: loss 2.127504\n",
      "iteration 1600 / 2000: loss 2.052036\n",
      "iteration 1700 / 2000: loss 2.096470\n",
      "iteration 1800 / 2000: loss 2.083645\n",
      "iteration 1900 / 2000: loss 2.119356\n",
      "That took 4.351391s\n",
      "iteration 0 / 2000: loss 1149.170668\n",
      "iteration 100 / 2000: loss 14.103163\n",
      "iteration 200 / 2000: loss 2.236642\n",
      "iteration 300 / 2000: loss 2.106070\n",
      "iteration 400 / 2000: loss 2.133104\n",
      "iteration 500 / 2000: loss 2.130257\n",
      "iteration 600 / 2000: loss 2.134815\n",
      "iteration 700 / 2000: loss 2.089388\n",
      "iteration 800 / 2000: loss 2.108189\n",
      "iteration 900 / 2000: loss 2.048319\n",
      "iteration 1000 / 2000: loss 2.069522\n",
      "iteration 1100 / 2000: loss 2.126455\n",
      "iteration 1200 / 2000: loss 2.119993\n",
      "iteration 1300 / 2000: loss 2.082382\n",
      "iteration 1400 / 2000: loss 2.108455\n",
      "iteration 1500 / 2000: loss 2.152088\n",
      "iteration 1600 / 2000: loss 2.112441\n",
      "iteration 1700 / 2000: loss 2.089648\n",
      "iteration 1800 / 2000: loss 2.163055\n",
      "iteration 1900 / 2000: loss 2.130124\n",
      "That took 4.342775s\n",
      "iteration 0 / 2000: loss 1347.882654\n",
      "iteration 100 / 2000: loss 8.692266\n",
      "iteration 200 / 2000: loss 2.121464\n",
      "iteration 300 / 2000: loss 2.121345\n",
      "iteration 400 / 2000: loss 2.179897\n",
      "iteration 500 / 2000: loss 2.105051\n",
      "iteration 600 / 2000: loss 2.159730\n",
      "iteration 700 / 2000: loss 2.158509\n",
      "iteration 800 / 2000: loss 2.133260\n",
      "iteration 900 / 2000: loss 2.133266\n",
      "iteration 1000 / 2000: loss 2.191728\n",
      "iteration 1100 / 2000: loss 2.110673\n",
      "iteration 1200 / 2000: loss 2.160672\n",
      "iteration 1300 / 2000: loss 2.153068\n",
      "iteration 1400 / 2000: loss 2.132286\n",
      "iteration 1500 / 2000: loss 2.117063\n",
      "iteration 1600 / 2000: loss 2.174220\n",
      "iteration 1700 / 2000: loss 2.153808\n",
      "iteration 1800 / 2000: loss 2.164153\n",
      "iteration 1900 / 2000: loss 2.160424\n",
      "That took 4.326478s\n",
      "iteration 0 / 2000: loss 1538.178654\n",
      "iteration 100 / 2000: loss 5.638525\n",
      "iteration 200 / 2000: loss 2.130104\n",
      "iteration 300 / 2000: loss 2.127870\n",
      "iteration 400 / 2000: loss 2.159889\n",
      "iteration 500 / 2000: loss 2.150129\n",
      "iteration 600 / 2000: loss 2.140531\n",
      "iteration 700 / 2000: loss 2.162962\n",
      "iteration 800 / 2000: loss 2.137139\n",
      "iteration 900 / 2000: loss 2.146670\n",
      "iteration 1000 / 2000: loss 2.147283\n",
      "iteration 1100 / 2000: loss 2.120285\n",
      "iteration 1200 / 2000: loss 2.145234\n",
      "iteration 1300 / 2000: loss 2.116361\n",
      "iteration 1400 / 2000: loss 2.169618\n",
      "iteration 1500 / 2000: loss 2.129723\n",
      "iteration 1600 / 2000: loss 2.156125\n",
      "iteration 1700 / 2000: loss 2.173590\n",
      "iteration 1800 / 2000: loss 2.117564\n",
      "iteration 1900 / 2000: loss 2.112483\n",
      "That took 4.361160s\n",
      "iteration 0 / 2000: loss 782.146349\n",
      "iteration 100 / 2000: loss 15.625891\n",
      "iteration 200 / 2000: loss 2.367101\n",
      "iteration 300 / 2000: loss 2.039658\n",
      "iteration 400 / 2000: loss 2.039287\n",
      "iteration 500 / 2000: loss 2.076357\n",
      "iteration 600 / 2000: loss 2.182683\n",
      "iteration 700 / 2000: loss 2.076218\n",
      "iteration 800 / 2000: loss 2.065408\n",
      "iteration 900 / 2000: loss 2.067690\n",
      "iteration 1000 / 2000: loss 2.065747\n",
      "iteration 1100 / 2000: loss 2.087186\n",
      "iteration 1200 / 2000: loss 2.088509\n",
      "iteration 1300 / 2000: loss 2.122409\n",
      "iteration 1400 / 2000: loss 2.009059\n",
      "iteration 1500 / 2000: loss 2.051122\n",
      "iteration 1600 / 2000: loss 2.067623\n",
      "iteration 1700 / 2000: loss 2.063215\n",
      "iteration 1800 / 2000: loss 2.110159\n",
      "iteration 1900 / 2000: loss 2.056594\n",
      "That took 5.040437s\n",
      "iteration 0 / 2000: loss 983.040700\n",
      "iteration 100 / 2000: loss 8.221769\n",
      "iteration 200 / 2000: loss 2.148277\n",
      "iteration 300 / 2000: loss 2.097342\n",
      "iteration 400 / 2000: loss 2.151951\n",
      "iteration 500 / 2000: loss 2.071133\n",
      "iteration 600 / 2000: loss 2.100694\n",
      "iteration 700 / 2000: loss 2.140173\n",
      "iteration 800 / 2000: loss 2.144909\n",
      "iteration 900 / 2000: loss 2.082162\n",
      "iteration 1000 / 2000: loss 2.099524\n",
      "iteration 1100 / 2000: loss 2.091095\n",
      "iteration 1200 / 2000: loss 2.123199\n",
      "iteration 1300 / 2000: loss 2.051477\n",
      "iteration 1400 / 2000: loss 2.127585\n",
      "iteration 1500 / 2000: loss 2.119131\n",
      "iteration 1600 / 2000: loss 2.136724\n",
      "iteration 1700 / 2000: loss 2.109601\n",
      "iteration 1800 / 2000: loss 2.123769\n",
      "iteration 1900 / 2000: loss 2.138506\n",
      "That took 4.982760s\n",
      "iteration 0 / 2000: loss 1147.844458\n",
      "iteration 100 / 2000: loss 4.665077\n",
      "iteration 200 / 2000: loss 2.153463\n",
      "iteration 300 / 2000: loss 2.130324\n",
      "iteration 400 / 2000: loss 2.170065\n",
      "iteration 500 / 2000: loss 2.117363\n",
      "iteration 600 / 2000: loss 2.148124\n",
      "iteration 700 / 2000: loss 2.073112\n",
      "iteration 800 / 2000: loss 2.058487\n",
      "iteration 900 / 2000: loss 2.126798\n",
      "iteration 1000 / 2000: loss 2.101413\n",
      "iteration 1100 / 2000: loss 2.146909\n",
      "iteration 1200 / 2000: loss 2.107700\n",
      "iteration 1300 / 2000: loss 2.132744\n",
      "iteration 1400 / 2000: loss 2.099330\n",
      "iteration 1500 / 2000: loss 2.107327\n",
      "iteration 1600 / 2000: loss 2.166613\n",
      "iteration 1700 / 2000: loss 2.148237\n",
      "iteration 1800 / 2000: loss 2.075569\n",
      "iteration 1900 / 2000: loss 2.115453\n",
      "That took 4.659812s\n",
      "iteration 0 / 2000: loss 1362.122044\n",
      "iteration 100 / 2000: loss 3.226999\n",
      "iteration 200 / 2000: loss 2.082308\n",
      "iteration 300 / 2000: loss 2.121767\n",
      "iteration 400 / 2000: loss 2.177591\n",
      "iteration 500 / 2000: loss 2.170142\n",
      "iteration 600 / 2000: loss 2.146507\n",
      "iteration 700 / 2000: loss 2.152107\n",
      "iteration 800 / 2000: loss 2.142650\n",
      "iteration 900 / 2000: loss 2.169792\n",
      "iteration 1000 / 2000: loss 2.156581\n",
      "iteration 1100 / 2000: loss 2.144971\n",
      "iteration 1200 / 2000: loss 2.102822\n",
      "iteration 1300 / 2000: loss 2.123328\n",
      "iteration 1400 / 2000: loss 2.198618\n",
      "iteration 1500 / 2000: loss 2.101104\n",
      "iteration 1600 / 2000: loss 2.097118\n",
      "iteration 1700 / 2000: loss 2.108196\n",
      "iteration 1800 / 2000: loss 2.162562\n",
      "iteration 1900 / 2000: loss 2.139569\n",
      "That took 4.355412s\n",
      "iteration 0 / 2000: loss 1527.297679\n",
      "iteration 100 / 2000: loss 2.577886\n",
      "iteration 200 / 2000: loss 2.153982\n",
      "iteration 300 / 2000: loss 2.149175\n",
      "iteration 400 / 2000: loss 2.148350\n",
      "iteration 500 / 2000: loss 2.173331\n",
      "iteration 600 / 2000: loss 2.125831\n",
      "iteration 700 / 2000: loss 2.171564\n",
      "iteration 800 / 2000: loss 2.191502\n",
      "iteration 900 / 2000: loss 2.167967\n",
      "iteration 1000 / 2000: loss 2.123367\n",
      "iteration 1100 / 2000: loss 2.121382\n",
      "iteration 1200 / 2000: loss 2.145247\n",
      "iteration 1300 / 2000: loss 2.120513\n",
      "iteration 1400 / 2000: loss 2.165623\n",
      "iteration 1500 / 2000: loss 2.177271\n",
      "iteration 1600 / 2000: loss 2.160786\n",
      "iteration 1700 / 2000: loss 2.206922\n",
      "iteration 1800 / 2000: loss 2.111472\n",
      "iteration 1900 / 2000: loss 2.098612\n",
      "That took 4.668081s\n",
      "iteration 0 / 2000: loss 761.817502\n",
      "iteration 100 / 2000: loss 6.794054\n",
      "iteration 200 / 2000: loss 2.079966\n",
      "iteration 300 / 2000: loss 2.033873\n",
      "iteration 400 / 2000: loss 2.106563\n",
      "iteration 500 / 2000: loss 2.129542\n",
      "iteration 600 / 2000: loss 2.091858\n",
      "iteration 700 / 2000: loss 2.076993\n",
      "iteration 800 / 2000: loss 2.070062\n",
      "iteration 900 / 2000: loss 2.132207\n",
      "iteration 1000 / 2000: loss 2.162449\n",
      "iteration 1100 / 2000: loss 2.078011\n",
      "iteration 1200 / 2000: loss 2.084653\n",
      "iteration 1300 / 2000: loss 2.062465\n",
      "iteration 1400 / 2000: loss 2.154270\n",
      "iteration 1500 / 2000: loss 2.081656\n",
      "iteration 1600 / 2000: loss 2.100036\n",
      "iteration 1700 / 2000: loss 2.101870\n",
      "iteration 1800 / 2000: loss 2.135512\n",
      "iteration 1900 / 2000: loss 2.121103\n",
      "That took 4.727353s\n",
      "iteration 0 / 2000: loss 986.048461\n",
      "iteration 100 / 2000: loss 3.770225\n",
      "iteration 200 / 2000: loss 2.082067\n",
      "iteration 300 / 2000: loss 2.111457\n",
      "iteration 400 / 2000: loss 2.114157\n",
      "iteration 500 / 2000: loss 2.120755\n",
      "iteration 600 / 2000: loss 2.128531\n",
      "iteration 700 / 2000: loss 2.094563\n",
      "iteration 800 / 2000: loss 2.091246\n",
      "iteration 900 / 2000: loss 2.108241\n",
      "iteration 1000 / 2000: loss 2.153700\n",
      "iteration 1100 / 2000: loss 2.109371\n",
      "iteration 1200 / 2000: loss 2.172297\n",
      "iteration 1300 / 2000: loss 2.088932\n",
      "iteration 1400 / 2000: loss 2.022669\n",
      "iteration 1500 / 2000: loss 2.130730\n",
      "iteration 1600 / 2000: loss 2.144821\n",
      "iteration 1700 / 2000: loss 2.108110\n",
      "iteration 1800 / 2000: loss 2.123695\n",
      "iteration 1900 / 2000: loss 2.110533\n",
      "That took 4.674297s\n",
      "iteration 0 / 2000: loss 1145.182181\n",
      "iteration 100 / 2000: loss 2.658730\n",
      "iteration 200 / 2000: loss 2.145331\n",
      "iteration 300 / 2000: loss 2.146352\n",
      "iteration 400 / 2000: loss 2.135175\n",
      "iteration 500 / 2000: loss 2.095048\n",
      "iteration 600 / 2000: loss 2.103486\n",
      "iteration 700 / 2000: loss 2.121579\n",
      "iteration 800 / 2000: loss 2.094432\n",
      "iteration 900 / 2000: loss 2.155194\n",
      "iteration 1000 / 2000: loss 2.117233\n",
      "iteration 1100 / 2000: loss 2.089658\n",
      "iteration 1200 / 2000: loss 2.104344\n",
      "iteration 1300 / 2000: loss 2.107025\n",
      "iteration 1400 / 2000: loss 2.102812\n",
      "iteration 1500 / 2000: loss 2.127796\n",
      "iteration 1600 / 2000: loss 2.105618\n",
      "iteration 1700 / 2000: loss 2.143909\n",
      "iteration 1800 / 2000: loss 2.135434\n",
      "iteration 1900 / 2000: loss 2.120599\n",
      "That took 4.888500s\n",
      "iteration 0 / 2000: loss 1355.521591\n",
      "iteration 100 / 2000: loss 2.295222\n",
      "iteration 200 / 2000: loss 2.130763\n",
      "iteration 300 / 2000: loss 2.131554\n",
      "iteration 400 / 2000: loss 2.128486\n",
      "iteration 500 / 2000: loss 2.182282\n",
      "iteration 600 / 2000: loss 2.142264\n",
      "iteration 700 / 2000: loss 2.133058\n",
      "iteration 800 / 2000: loss 2.167342\n",
      "iteration 900 / 2000: loss 2.147993\n",
      "iteration 1000 / 2000: loss 2.161062\n",
      "iteration 1100 / 2000: loss 2.132828\n",
      "iteration 1200 / 2000: loss 2.135772\n",
      "iteration 1300 / 2000: loss 2.145707\n",
      "iteration 1400 / 2000: loss 2.131613\n",
      "iteration 1500 / 2000: loss 2.185637\n",
      "iteration 1600 / 2000: loss 2.110957\n",
      "iteration 1700 / 2000: loss 2.107339\n",
      "iteration 1800 / 2000: loss 2.100539\n",
      "iteration 1900 / 2000: loss 2.145581\n",
      "That took 4.843543s\n",
      "iteration 0 / 2000: loss 1545.410877\n",
      "iteration 100 / 2000: loss 2.158293\n",
      "iteration 200 / 2000: loss 2.178410\n",
      "iteration 300 / 2000: loss 2.111191\n",
      "iteration 400 / 2000: loss 2.179584\n",
      "iteration 500 / 2000: loss 2.134826\n",
      "iteration 600 / 2000: loss 2.109423\n",
      "iteration 700 / 2000: loss 2.141572\n",
      "iteration 800 / 2000: loss 2.124526\n",
      "iteration 900 / 2000: loss 2.151610\n",
      "iteration 1000 / 2000: loss 2.135452\n",
      "iteration 1100 / 2000: loss 2.116522\n",
      "iteration 1200 / 2000: loss 2.139305\n",
      "iteration 1300 / 2000: loss 2.160087\n",
      "iteration 1400 / 2000: loss 2.108712\n",
      "iteration 1500 / 2000: loss 2.161594\n",
      "iteration 1600 / 2000: loss 2.157117\n",
      "iteration 1700 / 2000: loss 2.175989\n",
      "iteration 1800 / 2000: loss 2.133475\n",
      "iteration 1900 / 2000: loss 2.134759\n",
      "That took 4.771350s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-681944070d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Print out results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n\u001b[1;32m     55\u001b[0m                 lr, reg, train_accuracy, val_accuracy))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-5]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "lr_list = np.linspace(learning_rates[0], learning_rates[1], 5)\n",
    "reg_list = np.linspace(regularization_strengths[0], regularization_strengths[1], 5)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in lr_list:\n",
    "    for reg in reg_list:\n",
    "        softmax = Softmax()\n",
    "        tic = time.time()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                      num_iters=1500, verbose=True)\n",
    "        toc = time.time()\n",
    "        print(\"That took %fs\" %(toc-tic))\n",
    "        \n",
    "        # Predict for training values\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_train_acc = np.mean(y_train_pred == y_train)\n",
    "        \n",
    "        # Predict for test values\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        y_val_acc = np.mean(y_val_pred == y_val)\n",
    "        \n",
    "        results[lr, reg] = y_val_acc\n",
    "        \n",
    "        if y_val_acc > best_val:\n",
    "            best_val = y_val_acc\n",
    "            best_softmax = softmax\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.329000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
